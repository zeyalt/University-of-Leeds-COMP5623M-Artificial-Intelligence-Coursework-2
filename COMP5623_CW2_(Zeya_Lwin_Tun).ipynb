{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a31sFIyrHaXl"
   },
   "source": [
    "# COMP5623 Coursework on Image Caption Generation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZAJ8mr5hgOf3"
   },
   "source": [
    "### Import the necessary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VXuzOezCgLjl"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import cv2\n",
    "from nltk.probability import FreqDist\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk import tokenize\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "from torch.nn.utils.rnn import pack_padded_sequence\n",
    "from torch.utils.data import Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ylzZ2dCfVlJD"
   },
   "source": [
    "### Download dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e8s_mPJ39mk7"
   },
   "outputs": [],
   "source": [
    "# Check what the current directory is\n",
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L6L8H5DhHrvy"
   },
   "outputs": [],
   "source": [
    "# Check current directory\n",
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Bwg169wBCYhj"
   },
   "outputs": [],
   "source": [
    "# Download the two zip files into current directory, '/content'\n",
    "url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_Dataset.zip\"\n",
    "urllib.request.urlretrieve(url, \"Flickr8k_Dataset.zip\")\n",
    "\n",
    "url = \"https://github.com/jbrownlee/Datasets/releases/download/Flickr8k/Flickr8k_text.zip\"\n",
    "urllib.request.urlretrieve(url, \"Flickr8k_text.zip\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Je7pVrd5WAZw"
   },
   "source": [
    "### Create folders to store dataset and captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PgwbfB6DqZjk"
   },
   "outputs": [],
   "source": [
    "# Create a folder named 'data' in '/content'\n",
    "!mkdir data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "E3G1dcIdcMxn"
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "aPDLMsgocCnh"
   },
   "outputs": [],
   "source": [
    "# Enter the folder 'data' \n",
    "cd data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9_rsTS7McB5w"
   },
   "outputs": [],
   "source": [
    "# Inside the folder 'data', create two more folders, one named 'images' and another named 'captions'\n",
    "!mkdir images\n",
    "!mkdir captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbYQrfwgqs0Q"
   },
   "outputs": [],
   "source": [
    "# Go back to the previous directory, '/content/data'\n",
    "cd /content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3615QfSwWKjH"
   },
   "source": [
    "### Unzip datasets into specified folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QjFI_HNfFJNb"
   },
   "outputs": [],
   "source": [
    "# Unzip the zip file, 'Flickr8k_Dataset.zip' into '/content/data/images'\n",
    "!unzip Flickr8k_Dataset.zip -d /content/data/images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DYuRfHPysDc6"
   },
   "outputs": [],
   "source": [
    "# Unzip the zip file, 'Flickr8k_Dataset.zip' into '/content/data/captions'\n",
    "!unzip Flickr8k_text.zip -d /content/data/captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6OdxpA12IW0W"
   },
   "outputs": [],
   "source": [
    "# Enter the folder containing the images\n",
    "cd /content/data/images/Flicker8k_Dataset/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9BCSPd2zUAoT"
   },
   "source": [
    "### Define directory names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iXpWOFqFOXcc"
   },
   "outputs": [],
   "source": [
    "root = \"/content/data/\"\n",
    "caption_dir = root + \"captions/\"                       \n",
    "image_dir = root + \"images/Flicker8k_Dataset/\"                           \n",
    "token_file = \"Flickr8k.token.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MJ-Gxk7FUGNp"
   },
   "source": [
    "### Define a function to read in our ground truth text file, line by line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NHC0y7zaOXq8"
   },
   "outputs": [],
   "source": [
    "def read_lines(filepath):\n",
    "\n",
    "    \"\"\" Open the ground truth captions into memory, line by line. \"\"\"\n",
    "\n",
    "    file = open(filepath, 'r')\n",
    "    lines = []\n",
    "\n",
    "    while True: \n",
    "        # Get next line from file until there's no more\n",
    "        line = file.readline() \n",
    "        if not line: \n",
    "            break\n",
    "        lines.append(line.strip())\n",
    "    file.close() \n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D86cJx2yv81K"
   },
   "source": [
    "Read ground truth captions (5 per image), into memory: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9m-snsM2XHuu"
   },
   "outputs": [],
   "source": [
    "lines = read_lines(caption_dir + token_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tY6Vq-PadikP"
   },
   "source": [
    "Read the first five lines:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-IkK91ZuXNB2"
   },
   "outputs": [],
   "source": [
    "lines[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "poNOITgUO4Et"
   },
   "source": [
    "Delete the caption corresponding to the image ID \"2258277193_586949ec62.jpg\" because the image does not exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CINvgULOjuJ"
   },
   "outputs": [],
   "source": [
    "lines = [line for line in lines if \"2258277193_586949ec62.jpg\" not in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "AOuR6ilzOhDK"
   },
   "outputs": [],
   "source": [
    "print(\"Total number of captions:\", len(lines))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uicTTNzXUqbt"
   },
   "source": [
    "### Define a class called `Vocabulary()` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "95I8EKdbVJfS"
   },
   "source": [
    "Our vocabulary should consist of all the possible words which can be used, both as input into the model and as an output prediction. The model will not predict words which are not in our vocabulary. Every word in the vocabulary will have a unique integer starting from 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WzfvdSDWVL7t"
   },
   "source": [
    "The function add_word() checks if a particular word already exists in the vocabulary. If it does not exist, create an entry in the dictionary. If it already exists, do nothing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oksUJjLPwApA"
   },
   "outputs": [],
   "source": [
    "class Vocabulary(object): \n",
    "\n",
    "    \"\"\"Simple vocabulary wrapper which maps every unique word to an integer ID. \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Intially, set both the IDs and words to empty dictionaries.\n",
    "        self.word2idx = {}\n",
    "        self.idx2word = {}\n",
    "        self.idx = 0\n",
    "\n",
    "    def add_word(self, word):\n",
    "        # If the word does not already exist in the dictionary, add it\n",
    "        if not word in self.word2idx:\n",
    "            self.word2idx[word] = self.idx\n",
    "            self.idx2word[self.idx] = word\n",
    "            # Increment the ID for the next word\n",
    "            self.idx += 1\n",
    "\n",
    "    def __call__(self, word):\n",
    "        # If we try to access a word in the dictionary which does not exist, return the <unk> id\n",
    "        if not word in self.word2idx:\n",
    "            return self.word2idx['<unk>']\n",
    "        return self.word2idx[word]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.word2idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R4uDgXqNWXGl"
   },
   "source": [
    "### Clean the captions, extract all words and store them in a list called `words`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "s9pGEqFaVKI0"
   },
   "source": [
    "Use...\n",
    "\n",
    "*   `split()` function to split a string of text by a specific separator. \n",
    "*   `rstrip()` function to remove trailing whitespaces.\n",
    "*   `lower()` function to convert all words into lowercase.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VEQtthpXwEoY"
   },
   "source": [
    "Extract all the words from ```lines```, and create a list of them in a variable ```words```, for example:\n",
    "\n",
    "```words = [\"a\", \"an\", \"the\", \"cat\"... ]```\n",
    "\n",
    "No need to worry about duplicates.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qrCbLzC5WjN4"
   },
   "source": [
    "### Concatenate the cleaned captions into a single list called ```cleaned_captions```.\n",
    "\n",
    "Keeping the same order, concatenate all the cleaned words from each caption into a string again, and add them all to a list of strings ```cleaned_captions```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qviRZjGdR3Oq"
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store the words in the captions later\n",
    "words = []\n",
    "\n",
    "# Create an empty list to store cleaned captions later\n",
    "cleaned_captions = []\n",
    "\n",
    "for i in lines:\n",
    "  \n",
    "  # Split the image ID from the caption text\n",
    "  x = i.split(\"\\t\")\n",
    "  \n",
    "  # Define punctuation marks and remove them from the caption text\n",
    "  punctuations = '''!()-[]{};:'\"\\,<>./?@#$%^&*_~'''\n",
    "  no_punct = \"\"\n",
    "  for char in x[1]:\n",
    "    if char not in punctuations:\n",
    "      no_punct = no_punct + char\n",
    "    x[1] = no_punct \n",
    "  \n",
    "  # Convert the words to lower case, remove any trailing white spaces, split the words up into a list of words\n",
    "  y = x[1].lower().rstrip().split()\n",
    "\n",
    "  # Append the list of processed words into the empty list to form a single list of all words\n",
    "  words = words + y\n",
    "  \n",
    "  # Append each caption into the cleaned_captions list\n",
    "  separator = \" \"\n",
    "  cleaned_captions.append(separator.join(y))\n",
    "  \n",
    "# Sanity checks... \n",
    "print(\"Total number of words from all captions: \", len(words))\n",
    "print(\"Total number of unique words from all captions: \", len(set(words)))\n",
    "print(\"Total number of captions: \", len(cleaned_captions))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TUlhIDKsfP5i"
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "cleaned_captions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RoZL81T6fhP3"
   },
   "outputs": [],
   "source": [
    "# Sanity checks\n",
    "words[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "byXj6SuaTAh1"
   },
   "source": [
    "### Filter out words which appear 3 times or less  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5WI22ghNcrQd"
   },
   "outputs": [],
   "source": [
    "# Create a FreqDist object\n",
    "freq_words = FreqDist(words)\n",
    "\n",
    "# Create an empty dictionary to store filtered words later\n",
    "filtered_words = dict()\n",
    "\n",
    "# Filter out words which appear 3 times or less\n",
    "for (k, v) in freq_words.items():\n",
    "  if v >= 4:\n",
    "    filtered_words[k] = v\n",
    "\n",
    "# Check the length of the vocabulary containing unique words after filtering out infrequently used words \n",
    "print(\"Total number of words from all captions, after filtering: \", len(filtered_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "rJDDAIBmZPe_"
   },
   "source": [
    "### Add words to our vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ctwErx_ZwAzB"
   },
   "outputs": [],
   "source": [
    "# Create a vocab instance\n",
    "vocab = Vocabulary()\n",
    "\n",
    "# Add the token words first\n",
    "vocab.add_word('<pad>')\n",
    "vocab.add_word('<start>') \n",
    "vocab.add_word('<end>') \n",
    "vocab.add_word('<unk>')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xzEYIvJ-GA_G"
   },
   "source": [
    "Add the rest of the words from the parsed captions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "L5QIA9etFfuI"
   },
   "outputs": [],
   "source": [
    "# Add every unique word in filtered_words_dict into our vocabulary\n",
    "for word in filtered_words.keys():\n",
    "  vocab.add_word(word) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Zj99JT2XwA4-"
   },
   "outputs": [],
   "source": [
    "# Check position of a random word in our vocabulary\n",
    "vocab('air')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ChA2jcHxX0Eg"
   },
   "outputs": [],
   "source": [
    "# Check the size of our vocabulary\n",
    "print(\"Total number of words in our vocabulary:\", len(vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "27AXNNg6HsR5"
   },
   "source": [
    "### Store all image IDs in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "UGGnaDIRbZUs"
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store image IDs later\n",
    "image_ids = []\n",
    "\n",
    "for i in lines:\n",
    "\n",
    "  # Split the image ID from the caption text\n",
    "  x = i.split(\"\\t\")\n",
    "\n",
    "  # Chop off the index from the image IDs\n",
    "  y = x[0].split(\".\")\n",
    "\n",
    "  # Append the image IDs to list image_ids\n",
    "  image_ids.append(y[0])\n",
    "  \n",
    "print(\"Total number of Image IDs:\", len(image_ids))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0_FbII1VwVSg"
   },
   "source": [
    "### Create a dataframe for the image paths and captions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYQz4T3mwA2o"
   },
   "outputs": [],
   "source": [
    "# Create data for data frame\n",
    "data = {\n",
    "    'image_id': image_ids,\n",
    "    'path': [image_dir + image_id + \".jpg\" for image_id in image_ids],\n",
    "    'caption': cleaned_captions\n",
    "}\n",
    "\n",
    "# Create a data frame\n",
    "data_df = pd.DataFrame(data, columns=['image_id', 'path', 'caption'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "POB7UiJLwYsf"
   },
   "outputs": [],
   "source": [
    "# Check first five rows of data frame\n",
    "data_df.head(n=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNLQ0K-_weJy"
   },
   "source": [
    "### Define a class called `Flickr8k` for the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wqf2_F6YwakD"
   },
   "outputs": [],
   "source": [
    "class Flickr8k(Dataset):\n",
    "\n",
    "    \"\"\" Flickr8k custom dataset compatible with torch.utils.data.DataLoader. \"\"\"\n",
    "    \n",
    "    def __init__(self, df, vocab, transform=None):\n",
    "\n",
    "        \"\"\" Set the path for images, captions and vocabulary wrapper.\n",
    "        Args:\n",
    "            df: df containing image paths and captions.\n",
    "            vocab: vocabulary wrapper.\n",
    "            transform: image transformer.\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.vocab = vocab\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "      \n",
    "        \"\"\" Returns one data pair (image and caption). \"\"\"\n",
    "\n",
    "        vocab = self.vocab\n",
    "\n",
    "        caption = self.df['caption'][index]\n",
    "        img_id = self.df['image_id'][index]\n",
    "        path = self.df['path'][index]\n",
    "\n",
    "        image = Image.open(open(path, 'rb'))\n",
    "\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        # Convert caption (string) to word ids.\n",
    "        tokens = caption.split()\n",
    "        caption = []\n",
    "        \n",
    "        # Build the Tensor version of the caption, with token words\n",
    "        caption.append(vocab('<start>'))\n",
    "        caption.extend([vocab(token) for token in tokens])\n",
    "        caption.append(vocab('<end>'))\n",
    "        target = torch.Tensor(caption)\n",
    "        return image, target\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D-Wv4b7TdLFR"
   },
   "source": [
    "### Define a function called `caption_collate_fn()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-vkld_4CwkPO"
   },
   "source": [
    "We need to overwrite the default PyTorch ```collate_fn()``` because our ground truth captions are sequential data of varying lengths. The default ```collate_fn()``` does not support merging the captions with padding.\n",
    "\n",
    "You can read more about it here: https://pytorch.org/docs/stable/data.html#dataloader-collate-fn. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "P5YmKr9ewkqO"
   },
   "outputs": [],
   "source": [
    "def caption_collate_fn(data):\n",
    "\n",
    "    \"\"\" Creates mini-batch tensors from the list of tuples (image, caption).\n",
    "    Args:\n",
    "        data: list of tuple (image, caption). \n",
    "            - image: torch tensor of shape (3, 256, 256).\n",
    "            - caption: torch tensor of shape (?); variable length.\n",
    "    Returns:\n",
    "        images: torch tensor of shape (batch_size, 3, 256, 256).\n",
    "        targets: torch tensor of shape (batch_size, padded_length).\n",
    "        lengths: list; valid length for each padded caption.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Sort a data list by caption length from longest to shortest.\n",
    "    data.sort(key=lambda x: len(x[1]), reverse=True)\n",
    "    images, captions = zip(*data)\n",
    "\n",
    "    # Merge images (from tuple of 3D tensor to 4D tensor).\n",
    "    images = torch.stack(images, 0)\n",
    "\n",
    "    # Merge captions (from tuple of 1D tensor to 2D tensor).\n",
    "    lengths = [len(cap) for cap in captions]\n",
    "    targets = torch.zeros(len(captions), max(lengths)).long()\n",
    "    for i, cap in enumerate(captions):\n",
    "        end = lengths[i]\n",
    "        targets[i, :end] = cap[:end]        \n",
    "    return images, targets, lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SMAskuTydTE8"
   },
   "source": [
    "### Transform the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "XpRbVk6BFTGD"
   },
   "outputs": [],
   "source": [
    "from torchvision import transforms\n",
    "\n",
    "# Crop size matches the input dimensions expected by the pre-trained ResNet\n",
    "data_transform = transforms.Compose([ \n",
    "    transforms.Resize(224),\n",
    "    transforms.CenterCrop(224),  \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.485, 0.456, 0.406),   # Using ImageNet norms\n",
    "                         (0.229, 0.224, 0.225))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GgS9OpZ7FaAj"
   },
   "source": [
    "Initialising the datasets. The only twist is that every image has 5 ground truth captions, so each image appears five times in the dataframe. We don't want an image to appear in more than one set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QnTvR684GGVV"
   },
   "outputs": [],
   "source": [
    "unit_size = 5\n",
    "\n",
    "train_split = 0.95 # Defines the ratio of train/test data.\n",
    "\n",
    "# We didn't shuffle the dataframe yet so this works\n",
    "train_size = unit_size * round(len(data_df)*train_split / unit_size)\n",
    "\n",
    "dataset_train = Flickr8k(\n",
    "    df=data_df[:train_size].reset_index(drop=True),\n",
    "    vocab=vocab,\n",
    "    transform=data_transform,\n",
    ")\n",
    "\n",
    "# Define a dataframe for generation of caption later\n",
    "test_df=data_df[(train_size):].reset_index(drop=True)\n",
    "\n",
    "dataset_test = Flickr8k(\n",
    "    df=test_df,\n",
    "    vocab=vocab,\n",
    "    transform=data_transform,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b81Dt35feKfs"
   },
   "source": [
    "### Define train and test data loaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uWuWg72dGOq9"
   },
   "source": [
    "Write the dataloaders ```train_loader``` and ```test_loader``` - explicitly replacing the collate_fn:\n",
    "\n",
    "```train_loader = torch.utils.data.DataLoader(\n",
    "  ...,\n",
    "  collate_fn=caption_collate_fn\n",
    ")```\n",
    "\n",
    "Set train batch size to 128 and be sure to set ```shuffle=True```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KkNrIRbXGLFG"
   },
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=128, \n",
    "    shuffle=True,\n",
    "    num_workers=2,\n",
    "    collate_fn=caption_collate_fn\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=128, \n",
    "    shuffle=False,\n",
    "    num_workers=2,\n",
    "    collate_fn=caption_collate_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3bMCTui6mFsd"
   },
   "outputs": [],
   "source": [
    "print(\"Number of batches in train loader:\", len(train_loader))\n",
    "print(\"Number of batches in test loader:\", len(test_loader))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oXlf8lt5TF0N"
   },
   "source": [
    "## Specify encoder and decoder models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ls8lyXA2GTC0"
   },
   "outputs": [],
   "source": [
    "class EncoderCNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "\n",
    "        \"\"\"Load the pretrained ResNet-152 and replace top fc layer.\"\"\"\n",
    "\n",
    "        super(EncoderCNN, self).__init__()\n",
    "        resnet = models.resnet152(pretrained=True) # Pre-trained on ImageNet by default (with a download bar.) \n",
    "        layers = list(resnet.children())[:-1]      # Keep all layers except the last one \n",
    "\n",
    "        # Unpack the layers and create a new Sequential\n",
    "        self.resnet = nn.Sequential(*layers) # * means unpack the layers and pass them as separate arguments. \n",
    "        # Sequential takes a list of layers - building a new network with all layers except the last one.\n",
    "        \n",
    "        # We want a specific output size, which is the size of our embedding, so\n",
    "        # we feed our extracted features from the last fc layer (dimensions 1 x 1000)\n",
    "        # into a Linear layer to resize\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size) \n",
    "        \n",
    "        # Batch normalisation helps to speed up training\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "        \n",
    "    def forward(self, images):\n",
    "\n",
    "        \"\"\"Extract feature vectors from input images.\"\"\"\n",
    "        \n",
    "        # Complete graph here. Remember to put the ResNet layer in a with torch.no_grad() block\n",
    "        # don't touch the weights of the ResNet. They already know stuff. \n",
    "\n",
    "        # ResNet layer\n",
    "        with torch.no_grad(): \n",
    "          features = self.resnet(images) \n",
    "\n",
    "        # Flattening\n",
    "        features = features.reshape(features.size(0), -1) \n",
    "\n",
    "        # Fully-connected/linear layer\n",
    "        features = self.linear(features) \n",
    "\n",
    "        # Batch normalisation\n",
    "        features = self.bn(features) \n",
    "\n",
    "        return features\n",
    "\n",
    "class DecoderRNN(nn.Module):\n",
    "\n",
    "    def __init__(self, embed_size, hidden_size, vocab_size, num_layers, max_seq_length=20):\n",
    "\n",
    "        \"\"\"Set the hyper-parameters and build the layers.\"\"\"\n",
    "\n",
    "        super(DecoderRNN, self).__init__()\n",
    "        \n",
    "        self.embed = nn.Embedding(vocab_size, embed_size) \n",
    "        \n",
    "        # Choose RNN or LSTM:\n",
    "        self.rnn = nn.RNN(input_size=embed_size, \n",
    "                          hidden_size=hidden_size, \n",
    "                          num_layers=num_layers, \n",
    "                          nonlinearity='tanh', \n",
    "                          bias=True, \n",
    "                          batch_first=True, \n",
    "                          dropout=0, \n",
    "                          bidirectional=False) \n",
    "\n",
    "        # self.lstm = nn.LSTM(input_size=embed_size, \n",
    "        #                     hidden_size=hidden_size, \n",
    "        #                     num_layers=num_layers, \n",
    "        #                     bias=True, \n",
    "        #                     batch_first=True, \n",
    "        #                     dropout=0, \n",
    "        #                     bidirectional=False) \n",
    "\n",
    "        self.linear = nn.Linear(hidden_size, vocab_size) # produce the actual word output predictions which match the vocab size\n",
    "        self.max_seq_length = max_seq_length\n",
    "        \n",
    "    def forward(self, features, captions, lengths):\n",
    "      \n",
    "        \"\"\"Decode image feature vectors and generates captions.\"\"\"\n",
    "        \n",
    "        embeddings = self.embed(captions) # Converts the captions into the feature vectors, then add them into the features (from the images).\n",
    "        embeddings = torch.cat((features.unsqueeze(1), embeddings), 1)\n",
    "\n",
    "        # What is \"packing\" a padded sequence? Knows the length of the longest thing, splits into sub-batches, and each sub-batch will go into RNN together.\n",
    "        packed = pack_padded_sequence(embeddings, lengths, batch_first=True) \n",
    "\n",
    "        # Choose RNN or LSTM:\n",
    "        hiddens, _ = self.rnn(packed) \n",
    "        # hiddens, _ = self.lstm(packed) \n",
    "        \n",
    "        outputs = self.linear(hiddens[0])\n",
    "\n",
    "        return outputs\n",
    "    \n",
    "    # This function will run thru the RNN/LSTM for you, return a list of word IDs (integers), then use vocab to convert word IDs to words.\n",
    "    # Note: Run encoder.eval() before using encoder for forward pass for test images thru. \n",
    "    def sample(self, features, states=None):\n",
    "\n",
    "        \"\"\"Generate captions for given image features using greedy search.\"\"\"\n",
    "        \n",
    "        sampled_ids = []\n",
    "        inputs = features.unsqueeze(1)\n",
    "        \n",
    "        for i in range(self.max_seq_length):\n",
    "\n",
    "            # Choose RNN or LSTM:         \n",
    "            hiddens, states = self.rnn(inputs, states)          # hiddens: (batch_size, 1, hidden_size)\n",
    "            # hiddens, states = self.lstm(inputs, states)\n",
    "            \n",
    "            outputs = self.linear(hiddens.squeeze(1))            # outputs:  (batch_size, vocab_size)\n",
    "            _, predicted = outputs.max(1)                        # predicted: (batch_size)\n",
    "            sampled_ids.append(predicted)\n",
    "            inputs = self.embed(predicted)                       # inputs: (batch_size, embed_size)\n",
    "            inputs = inputs.unsqueeze(1)                         # inputs: (batch_size, 1, embed_size)\n",
    "\n",
    "        sampled_ids = torch.stack(sampled_ids, 1)                # sampled_ids: (batch_size, max_seq_length)\n",
    "        \n",
    "        return sampled_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C9-qtkkMTrtB"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dhecFOMRUgpe"
   },
   "source": [
    "## Set training parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Fd2-IX2Uer3"
   },
   "outputs": [],
   "source": [
    "embed_size = 256\n",
    "hidden_size = 512\n",
    "num_layers = 1\n",
    "learning_rate = 0.001\n",
    "num_epochs = 5\n",
    "log_step = 10\n",
    "save_step = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AlIwF6P8UgB4"
   },
   "source": [
    "Initialize the models and set the learning parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uxwDUlR2Uy7t"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Build the models\n",
    "encoder = EncoderCNN(embed_size).to(device)\n",
    "decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimisation will be on the parameters of BOTH the enocder and decoder,\n",
    "# but excluding the ResNet parameters, only the new added layers.\n",
    "params = list(\n",
    "    decoder.parameters()) + list(encoder.linear.parameters()) + list(encoder.bn.parameters()\n",
    ")\n",
    "\n",
    "optimizer = torch.optim.Adam(params, lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RUmSb2MHEZw3"
   },
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uS4oN21vNKu7"
   },
   "source": [
    "The loop to train the model. Feel free to put this in a function if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5M7KY9G3NI8l"
   },
   "outputs": [],
   "source": [
    "# Train the models\n",
    "total_step = len(train_loader)\n",
    "\n",
    "training_loss = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "\n",
    "    # Before Training\n",
    "    if epoch == 0:\n",
    "      print(\"=== START OF TRAINING ===\")\n",
    "      print(\"==> Saving 'encoder-at-checkpoint-%d.ckpt' and 'decoder-at-checkpoint-%d.ckpt'...\" % (epoch, epoch))\n",
    "      print(\"\\n\")\n",
    "      torch.save(encoder.state_dict(), 'encoder-at-checkpoint-{}.ckpt'.format(epoch))\n",
    "      torch.save(decoder.state_dict(), 'decoder-at-checkpoint-{}.ckpt'.format(epoch))\n",
    "\n",
    "    for i, (images, captions, lengths) in enumerate(train_loader):\n",
    "\n",
    "        # Set mini-batch dataset\n",
    "        images = images.to(device)\n",
    "        captions = captions.to(device)\n",
    "\n",
    "        # Packed as well as we'll compare to the decoder outputs\n",
    "        targets = pack_padded_sequence(captions, lengths, batch_first=True)[0]\n",
    "\n",
    "        # Forward, backward and optimize\n",
    "        features = encoder(images)\n",
    "        outputs = decoder(features, captions, lengths)\n",
    "\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Zero gradients for both networks\n",
    "        decoder.zero_grad()\n",
    "        encoder.zero_grad()\n",
    "\n",
    "        loss.backward() # For both networks\n",
    "        optimizer.step()\n",
    "\n",
    "        # Print log info every 10 batches \n",
    "        if i % log_step == 0:\n",
    "            print('Epoch [{}/{}], Batch [{}/{}], Loss: {:.4f}'\n",
    "                  .format(epoch+1, num_epochs, i, total_step, loss.item()))\n",
    "            training_loss.append(loss.item()) \n",
    "            \n",
    "    print(\"==> Saving 'encoder-at-checkpoint-%d.ckpt' and 'decoder-at-checkpoint-%d.ckpt'...\" % (epoch+1, epoch+1))\n",
    "    print(\"\\n\")\n",
    "    torch.save(encoder.state_dict(), 'encoder-at-checkpoint-{}.ckpt'.format(epoch+1))\n",
    "    torch.save(decoder.state_dict(), 'decoder-at-checkpoint-{}.ckpt'.format(epoch+1))\n",
    "\n",
    "print(\"=== END OF TRAINING ===\")\n",
    "\n",
    "# Plot training loss \n",
    "plt.plot(training_loss, label = \"Training Loss\")   \n",
    "plt.xlabel(\"Number of Mini-Batches\")\n",
    "plt.ylabel(\"Training Loss\")\n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZHMkLKnQo7Vk"
   },
   "source": [
    "## Define a function to load image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ORimKnTbkQ8s"
   },
   "outputs": [],
   "source": [
    "def load_image(image_path, transform=None):\n",
    "\n",
    "  \"\"\"\n",
    "  This function reads in an image path, transform the image and loads it.\n",
    "  \"\"\"\n",
    "\n",
    "  image = Image.open(image_path).convert('RGB')\n",
    "  image = image.resize([224, 224], Image.LANCZOS)\n",
    "  if transform is not None:\n",
    "    image = transform(image).unsqueeze(0)\n",
    "  return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eZ22jmlYpR8U"
   },
   "source": [
    "## Define a function to extract reference captions for an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zNGhzj5O17-s"
   },
   "outputs": [],
   "source": [
    "def extract_reference_captions(image_id):\n",
    "\n",
    "  \"\"\"\n",
    "  This function extracts the five reference captions given a particular image ID.\n",
    "  \"\"\"\n",
    "\n",
    "  reference_captions = []\n",
    "  reference_captions_split = []\n",
    "\n",
    "  for idx, line in enumerate(lines):\n",
    "    if image_id in line:\n",
    "      reference_captions_split.append(cleaned_captions[idx].split())\n",
    "      reference_captions.append(cleaned_captions[idx])\n",
    "      \n",
    "  return reference_captions_split, reference_captions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "12TZt2Y8o9zn"
   },
   "source": [
    "## Define a function to generate captions by inputing a filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lSEsPu54-9kV"
   },
   "outputs": [],
   "source": [
    "def generate_caption(filepath_image):\n",
    "\n",
    "  \"\"\"\n",
    "  This function generates captions of an image given its filepath.\n",
    "  \"\"\"\n",
    "\n",
    "  image = load_image(filepath_image, transform=data_transform)\n",
    "  image_tensor = image.to(device)\n",
    "  feature_image = encoder(image_tensor)\n",
    "  sampled_ids_image = decoder.sample(feature_image)\n",
    "  sampled_ids_image = sampled_ids_image[0].cpu().numpy()\n",
    "\n",
    "  # Convert word_ids to words\n",
    "  sampled_caption = []\n",
    "  for word_id in sampled_ids_image:\n",
    "    word = vocab.idx2word[word_id]\n",
    "    sampled_caption.append(word)\n",
    "    if word == '<end>':\n",
    "      break\n",
    "  \n",
    "  return sampled_caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Oao3O8stp1Nt"
   },
   "source": [
    "## Define function to remove tokens from generated captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "278iCpn19Yyy"
   },
   "outputs": [],
   "source": [
    "def clean_generated_captions(caption):\n",
    "  \n",
    "  \"\"\"\n",
    "  This function removes <pad>, <start>, <end>, <unk> from the generated captions for BLEU score computation.\n",
    "  \"\"\"\n",
    "\n",
    "  special_tokens = [\"<start>\", \"<end>\", \"<pad>\", \"<unk>\"]\n",
    "  \n",
    "  for i in range(len(special_tokens)):\n",
    "    for token in caption:\n",
    "      if token == special_tokens[i]:\n",
    "        caption.remove(special_tokens[i])  \n",
    "\n",
    "  return caption"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uYiQB7tYuBEd"
   },
   "source": [
    "## Create a dataframe to store lengths of captions of each image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6dBpPdZz1Sto"
   },
   "outputs": [],
   "source": [
    "# Create a list to store length of each caption\n",
    "caption_length_list = []\n",
    "\n",
    "# Iterate through each row in test_df and append the length of each caption to the list\n",
    "for idx, caption in enumerate(test_df['caption']):\n",
    "  caption_length_list.append(len(caption.split()))\n",
    "\n",
    "# Convert the list to dataframe\n",
    "caption_length_df = pd.DataFrame(caption_length_list, columns=['caption_length'])\n",
    "\n",
    "# Concatenate test_df with caption_length_df\n",
    "test_df2 = pd.concat([test_df, caption_length_df], axis=1)\n",
    "test_df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9DYRRcazfhEY"
   },
   "source": [
    "## Create a data frame to store average length of reference captions per image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ific_gIU2x-M"
   },
   "outputs": [],
   "source": [
    "test_df3 = test_df2.groupby('image_id').mean().reset_index(drop=False)\n",
    "test_df3.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "erUKPGyGwlK7"
   },
   "source": [
    "## Create an empty dataframe to store generated captions and BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NaQwbHRzpMWT"
   },
   "outputs": [],
   "source": [
    "test_df4 = pd.DataFrame(columns=['caption_0', 'BLEU_0', 'caption_1', 'BLEU_1', 'caption_2', 'BLEU_2', \n",
    "                                 'caption_3', 'BLEU_3', 'caption_4', 'BLEU_4', 'caption_5', 'BLEU_5'], index=range(0, 405))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7Trmi9drn3B8"
   },
   "source": [
    "## Define the paths of every image in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-TzS708qtPa"
   },
   "outputs": [],
   "source": [
    "root_image = \"/content/data/images/Flicker8k_Dataset/\"\n",
    "name_image = [image_id + \".jpg\" for image_id in list(test_df3['image_id'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXsFqUuCoBBA"
   },
   "source": [
    "## ***Store generated caption and BLEU scores in dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FcPIAL8L9h9g"
   },
   "outputs": [],
   "source": [
    "base_encoder = \"encoder-at-checkpoint-{}.ckpt\"\n",
    "base_decoder = \"decoder-at-checkpoint-{}.ckpt\"\n",
    "\n",
    "print(\"=== START OF CAPTION GENERATION ===\")\n",
    "\n",
    "# For loop to iterate through all images in the test set\n",
    "for idx, image in enumerate(name_image):\n",
    "\n",
    "  print(\"Index:\", idx)\n",
    "  reference_captions = extract_reference_captions(image)\n",
    "\n",
    "  # Create an empty list to store generated captions and BLEU scores for each image\n",
    "  x = []\n",
    "\n",
    "  # For loop to iterate through epochs for each image\n",
    "  for i in range(num_epochs+1):\n",
    "    #print(\"Checkpoint:\", i)\n",
    "    j = base_encoder.format(i)\n",
    "    k = base_decoder.format(i)\n",
    "    \n",
    "    # Build the models\n",
    "    encoder = EncoderCNN(embed_size).eval()\n",
    "    decoder = DecoderRNN(embed_size, hidden_size, len(vocab), num_layers)\n",
    "\n",
    "    encoder = encoder.to(device)\n",
    "    decoder = decoder.to(device)\n",
    "\n",
    "    encoder.load_state_dict(torch.load(j))\n",
    "    decoder.load_state_dict(torch.load(k))\n",
    "\n",
    "    # Generated caption at each checkpoint given an image file path\n",
    "    generated_caption_split = generate_caption(filepath_image=root_image+image) \n",
    "    generated_caption = ' '.join(generated_caption_split)\n",
    "    #print(\"Generated caption:\", generated_caption)\n",
    "    x.append(generated_caption)\n",
    " \n",
    "    # Get rid of special tokens in generated captions for BLEU score computation\n",
    "    candidate_caption = clean_generated_captions(generated_caption_split)\n",
    "\n",
    "    # Compute BLEU score\n",
    "    score = sentence_bleu(reference_captions[0], candidate_caption, weights=(0.15, 0.15, 0.30, 0.40))\n",
    "    #print(\"BLEU score:\", score)\n",
    "    x.append(score)\n",
    "\n",
    "  # Fill the empty dataframe, test_df4 with generated captions and BLEU scores of each image\n",
    "  test_df4.iloc[idx] = x\n",
    "\n",
    "print(\"=== END OF CAPTION GENERATION ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kzzyEJZlptTR"
   },
   "outputs": [],
   "source": [
    "test_df4.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QfzbO1Ilof_M"
   },
   "source": [
    "## Concatenate dataframes and save resulting dataframe as .csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b-xjWskBwz8r"
   },
   "outputs": [],
   "source": [
    "# Concatenate test_df3 with test_df4\n",
    "test_results_df = pd.concat([test_df3, test_df4], axis=1)\n",
    "test_results_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ImZ4vM9XpCr8"
   },
   "outputs": [],
   "source": [
    "# Specify file path to save csv file\n",
    "cd /content/data/images/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3foLC1LbSx5H"
   },
   "outputs": [],
   "source": [
    "# Save as .csv file\n",
    "test_results_df.to_csv(\"test_results_rnn.csv\")\n",
    "# test_results_df.to_csv(\"test_results_lstm.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i-IWHBn1S68-"
   },
   "source": [
    "# Comparing RNN vs LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "njpONdaTVYsi"
   },
   "outputs": [],
   "source": [
    "# Load .csv file\n",
    "test_results_df = pd.read_csv(\"test_results_rnn.csv\")\n",
    "# test_results_df = pd.read_csv(\"test_results_lstm.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JilZlItgsVSl"
   },
   "outputs": [],
   "source": [
    "# Subset the dataframe with only BLEU scores\n",
    "test_results_bleu = test_results_df[['image_id', 'caption_length', 'BLEU_0', 'BLEU_1', 'BLEU_2', 'BLEU_3', 'BLEU_4', 'BLEU_5']]\n",
    "#test_results_bleu = test_results_bleu[['BLEU_0', 'BLEU_1', 'BLEU_2', 'BLEU_3', 'BLEU_4', 'BLEU_5']].astype(float)\n",
    "test_results_bleu2 = test_results_bleu.iloc[:,1:].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QN7A3UP6lL-T"
   },
   "source": [
    "## Summary Statistics on BLEU Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zGtHYH3rUuB_"
   },
   "outputs": [],
   "source": [
    "test_results_bleu2.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "CIjh_Eg-lP8S"
   },
   "outputs": [],
   "source": [
    "# Average BLEU score at each checkpoint\n",
    "test_results_bleu2.describe().loc['mean'].iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOi3cOSYlQjD"
   },
   "outputs": [],
   "source": [
    "# Average BLEU score across all 6 checkpoints\n",
    "test_results_bleu2.describe().loc['mean'].iloc[1:].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1RsE4xralQq8"
   },
   "outputs": [],
   "source": [
    "# Average BLEU score across all checkpoints, excluding Checkpoint 0\n",
    "test_results_bleu2.describe().loc['mean'].iloc[2:].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_LKqqZ5qlYrh"
   },
   "source": [
    "## Define function that divides test set into intervals and computes BLEU scores for each interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1e4TgUbelbw-"
   },
   "outputs": [],
   "source": [
    "def average_bleu_by_caption_length(data, bin_size):\n",
    "    \n",
    "    \"\"\" \n",
    "    This function divides the data into bins according to caption lengths,\n",
    "    and calculates the average BLEU scores for each interval.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Length of shortest caption\n",
    "    caption_length_min = min(data['caption_length'])\n",
    "\n",
    "    # Length of longest caption\n",
    "    caption_length_max = max(data['caption_length'])\n",
    "\n",
    "    # Set interval\n",
    "    interval = (caption_length_max - caption_length_min) / bin_size\n",
    "\n",
    "    # Set breaks\n",
    "    breaks = np.arange(caption_length_min, caption_length_max+interval, interval)\n",
    "    \n",
    "    # Group data by caption_length according to interval breaks and computes average BLEU scores for each interval\n",
    "    bleu_by_caption_length = round(data.groupby(pd.cut(data[\"caption_length\"], breaks)).mean(), 3)\n",
    "    \n",
    "    # Rename columns\n",
    "    bleu_by_caption_length.columns = ['average_caption_length', 'BLEU_0', 'BLEU_1', 'BLEU_2', 'BLEU_3', 'BLEU_4',\n",
    "       'BLEU_5']\n",
    "    \n",
    "    return bleu_by_caption_length.reset_index(drop=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TvzTbdjRlumC"
   },
   "source": [
    "## Define function to plot BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8Q0RyL3plclz"
   },
   "outputs": [],
   "source": [
    "def plot_bleu_scores(data, bin_size):\n",
    "    \n",
    "  \"\"\" \n",
    "  This function plots BLEU scores across the checkpoints.\n",
    "  \"\"\"\n",
    "    checkpoints = np.arange(6)\n",
    "    legend_names = list(data['caption_length'])\n",
    "    palette = plt.get_cmap('Set2')\n",
    "    plt.figure(figsize=(8,6))\n",
    "    for i in range(bin_size):\n",
    "        plt.plot(checkpoints, data.iloc[i][2:], \n",
    "                 color=palette(i), \n",
    "                 label=str(legend_names[i]))\n",
    "    plt.xlabel('Checkpoint Number')\n",
    "    plt.ylabel('BLEU Score')\n",
    "    plt.title('Variation of BLEU Scores Across Checkpoints (RNN)')\n",
    "    # plt.title('Variation of BLEU Scores Across Checkpoints (LSTM)') # For LSTM\n",
    "    plt.legend(title=\"Interval Ranges for BLEU Scores\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8brdwapPl3Eu"
   },
   "source": [
    "## Plot BLEU scores at each checkpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kJVJfGw3lcZp"
   },
   "outputs": [],
   "source": [
    "# Set bin size\n",
    "bin_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "RLRRfTHIlcPs"
   },
   "outputs": [],
   "source": [
    "test_results_bleu_interval = average_bleu_by_caption_length(data=test_results_bleu2, bin_size=bin_size)\n",
    "test_results_bleu_interval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SqfeiT35l8an"
   },
   "outputs": [],
   "source": [
    "plot_bleu_scores(data=test_results_bleu_interval, bin_size=bin_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AMO4--zHw-gm"
   },
   "source": [
    "## Determine checkpoints where BLEU scores are minimum and maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "racrS8SAsVWd"
   },
   "outputs": [],
   "source": [
    "# Create an empty dataframe\n",
    "test_df5 = pd.DataFrame(columns=['min_BLEU_index', 'max_BLEU_index'], index=range(0, 405))\n",
    "\n",
    "# Iterate through all images and store the index corresponding to minimum and maximum BLEU scores in the dataframe\n",
    "for idx, image_id in enumerate(test_results_bleu['image_id']):\n",
    "    \n",
    "    BLEU_min = np.where(test_results_bleu.iloc[idx].iloc[2:] == min(test_results_bleu.iloc[idx].iloc[2:]))[0].astype(int)[0]\n",
    "    test_df5.iloc[idx][0] = BLEU_min\n",
    "    \n",
    "    BLEU_max = np.where(test_results_bleu.iloc[idx].iloc[2:] == max(test_results_bleu.iloc[idx].iloc[2:]))[0].astype(int)[0]\n",
    "    test_df5.iloc[idx][1] = BLEU_max\n",
    "\n",
    "test_df5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uCtKyYUrsVdI"
   },
   "outputs": [],
   "source": [
    "test_df5['max_BLEU_index'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dVcrfSuvegMa"
   },
   "outputs": [],
   "source": [
    "test_df5['min_BLEU_index'].value_counts(sort=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "apix962SsViH"
   },
   "outputs": [],
   "source": [
    "max_BLEU = list(test_df5['max_BLEU_index'].value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rRlYFrZxmh6Z"
   },
   "outputs": [],
   "source": [
    "min_BLEU = list(test_df5['min_BLEU_index'].value_counts(sort=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EvAzEqP3m7OH"
   },
   "source": [
    "## Define function that plots bar graphs of number of checkpoints corresponding to minimum or maximum BLEU scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qm1Og2P-msjh"
   },
   "outputs": [],
   "source": [
    "def plot_bar_graph(bleu_list, min_or_max):\n",
    "    \n",
    "    \"\"\"\n",
    "    This function plots bar graphs of number of checkpoints corresponding to minimum or maximum BLEU scores\n",
    "    \"\"\"\n",
    "\n",
    "    if min_or_max == \"Minimum\":\n",
    "        x = 0\n",
    "    else:\n",
    "        x = 1\n",
    "    \n",
    "    plt.figure(figsize=(8,6))\n",
    "    \n",
    "    for i in range(x,6):\n",
    "        plt.bar(i, height=bleu_list[i-1], color=(0.2, 0.4, 0.6, 0.6)) if x == 1 else plt.bar(i, height=bleu_list[i], color=(0.2, 0.4, 0.6, 0.6))\n",
    "        plt.xlabel('Checkpoint Number')\n",
    "        plt.ylabel('Number of Test Images')\n",
    "        plt.annotate(bleu_list[i-1], (i-0.1, bleu_list[i-1]+2)) if x == 1 else plt.annotate(bleu_list[i], (i-0.1, bleu_list[i]+2))\n",
    "        plot_title = \"Checkpoints Corresponding to \" + min_or_max + \" BLEU Score Across Test Set\"\n",
    "        plt.title(plot_title)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "80XD-Vr4nRGD"
   },
   "source": [
    "## Plot bar graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8bfAwHN1msY6"
   },
   "outputs": [],
   "source": [
    "plot_bar_graph(bleu_list=min_BLEU, min_or_max=\"Minimum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "a1a55o0BmroO"
   },
   "outputs": [],
   "source": [
    "plot_bar_graph(bleu_list=max_BLEU, min_or_max=\"Maximum\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "COMP5623_CW2_Starter.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
